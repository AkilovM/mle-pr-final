{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1a26b5",
   "metadata": {},
   "source": [
    "Эксперименты, Пайплайн обработки данных, Построение модели\n",
    "-\n",
    "\n",
    "Baseline: чистим, кодируем, нормируем, обучаем, оцениваем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae739dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5897/2181999733.py:8: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('dataset/train_ver2.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pickle\n",
    "#from category_encoders import CatBoostEncoder\n",
    "\n",
    "df = pd.read_csv('dataset/train_ver2.csv')\n",
    "df = df.drop(['tipodom'], axis=1)\n",
    "df_last_month = df[df['fecha_dato'] == \"2016-05-28\"]\n",
    "products = [x for x in df.columns if '_ult1' in x]\n",
    "\n",
    "def baseline_data_transform(df: pd.DataFrame, keep_user_ids=False):\n",
    "    \n",
    "    def remove_outliers(data: pd.DataFrame, outliers_columns: list):\n",
    "        num_cols = outliers_columns\n",
    "        threshold = 1.5\n",
    "        #potential_outliers = pd.DataFrame()\n",
    "\n",
    "        for col in num_cols:\n",
    "            Q1 = data[col].quantile(0.25)\n",
    "            Q3 = data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            margin = threshold * IQR\n",
    "            lower = Q1 - margin\n",
    "            upper = Q3 + margin\n",
    "            #potential_outliers[col] = ~data[col].between(lower, upper)\n",
    "            data = data[data[col].between(lower, upper)]\n",
    "        #return potential_outliers\n",
    "        return data\n",
    "\n",
    "    # берём только последний месяц\n",
    "    df = df[df['fecha_dato'] == \"2016-05-28\"]\n",
    "\n",
    "    # удалить столбцы\n",
    "    if keep_user_ids:\n",
    "        df = df.drop(['fecha_dato', 'ult_fec_cli_1t', 'conyuemp'], axis=1)\n",
    "    else:\n",
    "        df = df.drop(['fecha_dato', 'ncodpers', 'ult_fec_cli_1t', 'conyuemp'], axis=1)\n",
    "\n",
    "    # удалить строки c пропусками\n",
    "    for col in df.columns.to_list():\n",
    "        if col not in ['renta']:\n",
    "            df = df[df[col].notna()]\n",
    "    \n",
    "    # заполнить пропуски в ренте нулями\n",
    "    df['renta'] = df['renta'].fillna(0.0)\n",
    "\n",
    "    # age строки перевести в int, почистить от ' NA'\n",
    "    #TODO\n",
    "    df['age'] = df['age'].astype(int)\n",
    "\n",
    "    # fecha_alta перевести в int число секунд с начала эпохи\n",
    "    df['fecha_alta'] = pd.to_datetime(df['fecha_alta']).astype(int) / 10**9\n",
    "    \n",
    "    # antiguedad строки конвертировать в int, и почистить от значений типа '     NA', -999999.\n",
    "    #TODO\n",
    "    df = df[~df['antiguedad'] < 0]\n",
    "    df['antiguedad'] = df['antiguedad'].astype(int)\n",
    "\n",
    "    # indrel_1mes намешаны разные типы, надо привести к одному типу 2.0 '2.0' '2' итп\n",
    "    #TODO\n",
    "    # решил в baseline удалить столбец\n",
    "    df = df.drop(['indrel_1mes'], axis=1)\n",
    "\n",
    "    # убираем выбросы\n",
    "    df = remove_outliers(df, ['age', 'renta', 'fecha_alta'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = baseline_data_transform(df_last_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6f682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    data.drop(products, axis=1),\n",
    "    data[products],\n",
    "    #stratify=data[products],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "categ_binary_vals = ['sexo', 'ind_nuevo', 'indrel', 'indresi', 'indext', 'indfall', 'ind_actividad_cliente']\n",
    "categ_vals = ['ind_empleado', 'pais_residencia', 'tiprel_1mes', 'canal_entrada', 'cod_prov', 'nomprov', 'segmento'] # indrel_1mes\n",
    "numeric_vals = ['age', 'fecha_alta', 'antiguedad', 'renta']\n",
    "\n",
    "# категориальные признаки решил закодировать через признак renta чтобы не было утечки\n",
    "def encode_cat_via_renta(df: pd.DataFrame):\n",
    "    global_mean = df['renta'].mean()\n",
    "    alpha = 100  # Чем больше alpha, тем сильнее сглаживание к global_mean\n",
    "    for cat in categ_vals:\n",
    "        # Формула: (mean_in_category * n_samples + global_mean * alpha) / (n_samples + alpha)\n",
    "        category_stats = df.groupby(cat)['renta'].agg(['mean', 'count'])\n",
    "        encoded_values = (\n",
    "            (category_stats['mean'] * category_stats['count'] + global_mean * alpha) / \n",
    "            (category_stats['count'] + alpha)\n",
    "        )\n",
    "        # Заменяем категориальные значения на закодированные\n",
    "        df[cat] = df[cat].map(encoded_values)\n",
    "    scaler = StandardScaler()\n",
    "    # нормируем\n",
    "    df[categ_vals] = scaler.fit_transform(df[categ_vals])\n",
    "    return df\n",
    "\n",
    "X_tr = encode_cat_via_renta(X_tr)\n",
    "X_val = encode_cat_via_renta(X_val)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('binary', OneHotEncoder(drop='if_binary'), categ_binary_vals),\n",
    "        #('cat', ?, categ_vals),\n",
    "        ('num', StandardScaler(), numeric_vals),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_tr_transformed = preprocessor.fit_transform(X_tr, y_tr)\n",
    "X_val_transformed = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09715bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   32.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   10.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   27.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   18.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   19.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   17.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    9.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   10.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   16.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   26.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   14.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   11.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   14.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   11.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   23.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   18.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   16.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   19.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   23.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        23\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.78      0.81      0.79    132622\n",
      "           3       0.00      0.00      0.00        71\n",
      "           4       0.42      0.12      0.18     17118\n",
      "           5       0.94      0.92      0.93      1784\n",
      "           6       0.51      0.26      0.34      1858\n",
      "           7       0.55      0.35      0.43     22299\n",
      "           8       0.51      0.16      0.24      7582\n",
      "           9       0.00      0.00      0.00        70\n",
      "          10       0.00      0.00      0.00       242\n",
      "          11       0.55      0.27      0.37      7327\n",
      "          12       0.55      0.24      0.34     17592\n",
      "          13       0.34      0.03      0.05      3341\n",
      "          14       0.47      0.01      0.01      1055\n",
      "          15       0.31      0.01      0.02      1638\n",
      "          16       0.75      0.01      0.03       454\n",
      "          17       0.33      0.04      0.06     10351\n",
      "          18       0.30      0.03      0.06      8013\n",
      "          19       0.30      0.03      0.06      4776\n",
      "          20       0.00      0.00      0.00       617\n",
      "          21       0.40      0.08      0.13     11360\n",
      "          22       0.40      0.08      0.13     12374\n",
      "          23       0.48      0.28      0.36     26369\n",
      "\n",
      "   micro avg       0.70      0.47      0.56    288940\n",
      "   macro avg       0.37      0.16      0.19    288940\n",
      "weighted avg       0.61      0.47      0.50    288940\n",
      " samples avg       0.51      0.46      0.47    288940\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = MultiOutputClassifier(RandomForestClassifier(\n",
    "    n_estimators=50,  # Уменьшите, если долго (начните с 50)\n",
    "    verbose=1,         # Вывод лога обучения\n",
    "    n_jobs=-1          # Использовать все ядра CPU\n",
    "))\n",
    "model.fit(X_tr_transformed, y_tr)\n",
    "\n",
    "predictions = model.predict(X_val_transformed)\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2eb83",
   "metadata": {},
   "source": [
    "Смотрим f1-score:\n",
    "- macro avg = 0.19 - модель плохо справляется с большинством классов.\n",
    "- weighted avg = 0.50 - с учетом количества в каждом классе модель справляется лучше на классах покрупнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51298a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Сохраняем\n",
    "with open('model_baseline_RandomForestClassification.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d22f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Загружаем\n",
    "with open('model_baseline_RandomForestClassification.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27501c",
   "metadata": {},
   "source": [
    "Залогируем baseline модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run baseline at: http://127.0.0.1:5000/#/experiments/1/runs/e8019caba6c544488ecbb8c8aa68584b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "mlflow_tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_registry_uri(mlflow_tracking_uri)\n",
    "\n",
    "EXPERIMENT_NAME = \"spain_bank\"\n",
    "RUN_NAME = \"baseline\"\n",
    "\n",
    "metrics = {'f1-score macro avg': 0.19,\n",
    "           'f1-score weighted avg': 0.50}\n",
    "signature = mlflow.models.infer_signature(X_val_transformed, predictions)\n",
    "\n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "if not experiment_id:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME) \n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    # model_info = mlflow.sklearn.log_model(\n",
    "    #     sk_model=model, \n",
    "    #     artifact_path='models',\n",
    "    #     await_registration_for=60,\n",
    "    #     signature=signature,\n",
    "    #     registered_model_name='model_spain_bank_baseline',\n",
    "    #     code_paths=['experiments.ipynb'],\n",
    "    #     pip_requirements='requirements.txt'\n",
    "    # )\n",
    "    \n",
    "    # к сожалению воркер виснет при логировании модели.\n",
    "    # сохраним только метрики, а модель если что останется файлом .pkl\n",
    "    mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f2215",
   "metadata": {},
   "source": [
    "Модель вышла размером 4.7 ГБ, и одно предсказание выполняется долго.\n",
    "\n",
    "Обучим другую полегче:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ccaed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        23\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.74      0.82      0.78    132622\n",
      "           3       0.00      0.00      0.00        71\n",
      "           4       0.00      0.00      0.00     17118\n",
      "           5       0.86      0.87      0.87      1784\n",
      "           6       0.29      0.00      0.01      1858\n",
      "           7       0.37      0.16      0.22     22299\n",
      "           8       0.00      0.00      0.00      7582\n",
      "           9       0.00      0.00      0.00        70\n",
      "          10       0.00      0.00      0.00       242\n",
      "          11       0.59      0.23      0.33      7327\n",
      "          12       0.53      0.19      0.27     17592\n",
      "          13       1.00      0.00      0.00      3341\n",
      "          14       0.00      0.00      0.00      1055\n",
      "          15       0.00      0.00      0.00      1638\n",
      "          16       0.00      0.00      0.00       454\n",
      "          17       0.47      0.00      0.00     10351\n",
      "          18       0.64      0.00      0.00      8013\n",
      "          19       0.48      0.00      0.01      4776\n",
      "          20       0.00      0.00      0.00       617\n",
      "          21       0.00      0.00      0.00     11360\n",
      "          22       0.00      0.00      0.00     12374\n",
      "          23       0.62      0.04      0.08     26369\n",
      "\n",
      "   micro avg       0.71      0.42      0.52    288940\n",
      "   macro avg       0.27      0.10      0.11    288940\n",
      "weighted avg       0.53      0.42      0.41    288940\n",
      " samples avg       0.52      0.44      0.46    288940\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/mle-user/mle_projects/sprint-projects/mle-pr-final/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = MultiOutputClassifier(\n",
    "    LogisticRegression(max_iter=1000, solver=\"lbfgs\", n_jobs=-1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_tr_transformed, y_tr)\n",
    "\n",
    "predictions = model.predict(X_val_transformed)\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем\n",
    "with open('model_baseline_LogReg.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем\n",
    "with open('model_baseline_LogReg.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4572f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/28 12:30:48 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Successfully registered model 'model_spain_bank_baseline_logreg'.\n",
      "2025/07/28 12:30:49 INFO mlflow.store.model_registry.abstract_store: Waiting up to 60 seconds for model version to finish creation. Model name: model_spain_bank_baseline_logreg, version 1\n",
      "Created version '1' of model 'model_spain_bank_baseline_logreg'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run baseline_logreg at: http://127.0.0.1:5000/#/experiments/1/runs/aa24b8c063c3471eb1c0eaeef2dde3b0\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "mlflow_tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_registry_uri(mlflow_tracking_uri)\n",
    "\n",
    "EXPERIMENT_NAME = \"spain_bank\"\n",
    "RUN_NAME = \"baseline_logreg\"\n",
    "\n",
    "metrics = {'f1-score macro avg': 0.11,\n",
    "           'f1-score weighted avg': 0.41}\n",
    "signature = mlflow.models.infer_signature(X_val_transformed, predictions)\n",
    "\n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "if not experiment_id:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME) \n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model, \n",
    "        artifact_path='models',\n",
    "        await_registration_for=60,\n",
    "        signature=signature,\n",
    "        registered_model_name='model_spain_bank_baseline_logreg',\n",
    "        code_paths=['experiments.ipynb'],\n",
    "        pip_requirements='requirements.txt'\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba97c0",
   "metadata": {},
   "source": [
    "Качество чуть хуже, но модель гораздо легче и быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c86942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в продакшене понадобится информация для предсказаний и информативного вывода\n",
    "def pack_prod_obj():\n",
    "    products_spanish_to_ru_dict = {\n",
    "        'ind_ahor_fin_ult1':\t'Сберегательный счёт',\n",
    "        'ind_aval_fin_ult1':\t'Банковская гарантия',\n",
    "        'ind_cco_fin_ult1':\t'Текущие счета',\n",
    "        'ind_cder_fin_ult1':\t'Деривативный счёт',\n",
    "        'ind_cno_fin_ult1':\t'Зарплатный проект',\n",
    "        'ind_ctju_fin_ult1':\t'Детский счёт',\n",
    "        'ind_ctma_fin_ult1':\t'Особый счёт 3',\n",
    "        'ind_ctop_fin_ult1':\t'Особый счёт',\n",
    "        'ind_ctpp_fin_ult1':\t'Особый счёт 2',\n",
    "        'ind_deco_fin_ult1':\t'Краткосрочный депозит',\n",
    "        'ind_deme_fin_ult1':\t'Среднесрочный депозит',\n",
    "        'ind_dela_fin_ult1':\t'Долгосрочный депозит',\n",
    "        'ind_ecue_fin_ult1':\t'Цифровой счёт',\n",
    "        'ind_fond_fin_ult1':\t'Денежный средства',\n",
    "        'ind_hip_fin_ult1':\t'Ипотека',\n",
    "        'ind_plan_fin_ult1':\t'Пенсионный план',\n",
    "        'ind_pres_fin_ult1':\t'Кредит',\n",
    "        'ind_reca_fin_ult1':\t'Налоговый счёт',\n",
    "        'ind_tjcr_fin_ult1':\t'Кредитная карта',\n",
    "        'ind_valo_fin_ult1':\t'Ценные бумаги',\n",
    "        'ind_viv_fin_ult1':\t'Домашний счёт',\n",
    "        'ind_nomina_ult1':\t'Аккаунт для выплаты зарплаты',\n",
    "        'ind_nom_pens_ult1':\t'Аккаунт для пенсионных обязательств',\n",
    "        'ind_recibo_ult1':\t'Дебетовый аккаунт',\n",
    "    }\n",
    "    data_for_user_ids = baseline_data_transform(df_last_month, keep_user_ids=True)\n",
    "    user_id_order = data_for_user_ids['ncodpers'].to_list()\n",
    "\n",
    "    # data.drop(products, axis=1),\n",
    "    # data[products],\n",
    "\n",
    "    X_prod = encode_cat_via_renta(data.drop(products, axis=1))\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "            ('binary', OneHotEncoder(drop='if_binary'), categ_binary_vals),\n",
    "            #('cat', ?, categ_vals),\n",
    "            ('num', StandardScaler(), numeric_vals),\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    X_prod_transformed = preprocessor.fit_transform(X_prod, data[products])\n",
    "\n",
    "    prod_obj = {\n",
    "        'products_spanish_to_ru_dict': products_spanish_to_ru_dict,\n",
    "        'user_id_order': user_id_order,\n",
    "        'products': products,\n",
    "        'features': X_prod_transformed,\n",
    "    }\n",
    "    return prod_obj\n",
    "\n",
    "prod_obj = pack_prod_obj()\n",
    "\n",
    "# Сохраняем\n",
    "with open('prod_obj.pkl', 'wb') as f:\n",
    "    pickle.dump(prod_obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb969378",
   "metadata": {},
   "source": [
    "Advanced\n",
    "-\n",
    "TODO\n",
    "\n",
    "Времени на эксперименты почти не осталось, поэтому отправляю текущее решение на проверку."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe3b2f",
   "metadata": {},
   "source": [
    "Эксперименты:\n",
    "- перестроить таргеты (см. EDA)\n",
    "- переделать датасет так, чтобы признаки n-го месяца лежали с таргетами n+1-го месяца;\n",
    "- сгенерировать признак - динамика поля `renta` за последние n месяцев;\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data):\n",
    "    feature_cols = data.columns.drop('customer_id').tolist()\n",
    "    is_duplicated_features = data.duplicated(subset=feature_cols, keep=False)\n",
    "    data = data[~is_duplicated_features].reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def fill_missing_values(data):\n",
    "\n",
    "    cols_with_nans = data.isnull().sum()\n",
    "    cols_with_nans = cols_with_nans[cols_with_nans > 0].index.drop('end_date')\n",
    "\n",
    "    for col in cols_with_nans:\n",
    "\n",
    "        if data[col].dtype in [float, int]:\n",
    "            fill_value = data[col].mean()\n",
    "        elif data[col].dtype == 'object':\n",
    "            fill_value = data[col].mode().iloc[0]\n",
    "\n",
    "        data[col] = data[col].fillna(fill_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_outliers(data):\n",
    "    num_cols = data.select_dtypes(['float']).columns\n",
    "    threshold = 1.5\n",
    "    potential_outliers = pd.DataFrame()\n",
    "\n",
    "    for col in num_cols:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        margin = threshold * IQR\n",
    "        lower = Q1 - margin\n",
    "        upper = Q3 + margin\n",
    "        potential_outliers[col] = ~data[col].between(lower, upper)\n",
    "    return potential_outliers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
